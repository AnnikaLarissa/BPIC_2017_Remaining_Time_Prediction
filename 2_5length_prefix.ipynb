{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "This notebook filters the data and saves training and test data in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import pandas as pd\n",
    "\n",
    "# import pm4py library to work with XES logs and process mining\n",
    "import pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = pm4py.read_xes(\"data/BPI_Challenge_2017.xes.gz\")\n",
    "log_df = pm4py.convert_to_dataframe(log)\n",
    "log_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event index in trace\n",
    "\n",
    "We add to each event in the log its position in their trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column \"event_index_in_trace\"\n",
    "# which indicates the 1st, 2nd ... event in the trace\n",
    "log_df = log_df.sort_values(by=[\"case:concept:name\", \"time:timestamp\"])\n",
    "log_df[\"event_index_in_trace\"] = log_df.groupby(\"case:concept:name\").cumcount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining time\n",
    "\n",
    "Here we calculate the remaining time per trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which indicates time from that event until the last event in the trace\n",
    "log_df[\"time:timestamp\"] = pd.to_datetime(log_df[\"time:timestamp\"], utc=True)\n",
    "log_df[\"remaining_time\"] = log_df.groupby(\"case:concept:name\")[\"time:timestamp\"].transform(lambda x: x.max() - x).dt.total_seconds() / (24 * 60 * 60)  # convert to float days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time being executed\n",
    "\n",
    "We also will store the time that the trace is being executed from the first event in the trace until the current event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df[\"time:timestamp\"] = pd.to_datetime(log_df[\"time:timestamp\"], utc=True)\n",
    "\n",
    "# Time in execution: time that has been the trace in execution from the first event\n",
    "log_df[\"execution_time\"] = log_df.groupby(\"case:concept:name\")[\"time:timestamp\"].transform(lambda x: x - x.min()).dt.total_seconds() / (24 * 60 * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the data 5 prefix length\n",
    "\n",
    "in joins of 5 events of traces that happens following to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test\n",
    "\n",
    "Using the pm4py.split_train_test resulted in traces in train that ended after the start of traces in test unfortunately. This is not a good split, so we implement it manually by sorting traces on timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = log_df.sort_values(by=[\"case:concept:name\", \"time:timestamp\"])\n",
    "trace_start_df = log_df[[\"case:concept:name\", \"time:timestamp\"]].groupby([\"case:concept:name\"]).min()\n",
    "trace_end_df = log_df[[\"case:concept:name\", \"time:timestamp\"]].groupby([\"case:concept:name\"]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the last 10% of the traces as test set\n",
    "test_size = round(len(trace_start_df)*0.1)\n",
    "test_cases = trace_start_df.sort_values(\"time:timestamp\").tail(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train cases must end before test cases start\n",
    "train_cases = trace_end_df[trace_end_df[\"time:timestamp\"] < test_cases[\"time:timestamp\"].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = log_df[log_df[\"case:concept:name\"].isin(train_cases.index)]\n",
    "test_df = log_df[log_df[\"case:concept:name\"].isin(test_cases.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check that the timestamps don't overlap\n",
    "# all traces in train must end before the start of traces in test\n",
    "print(train_df[\"time:timestamp\"].max())\n",
    "print(test_df[\"time:timestamp\"].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature encoding\n",
    "\n",
    "For now we use the basic feature encoding from pm4py, but we want to experiment with using complex index encoding, where we encode the previous 10 activities (or add padding). Furthermore, we add the index of the activity in the log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features we are going to encode\n",
    "columns_to_encode = ['Action', 'concept:name', 'case:LoanGoal']\n",
    "\n",
    "# one-hot encode the data\n",
    "train_df_encode = pd.get_dummies(train_df[columns_to_encode], dtype=int)\n",
    "test_df_encode = pd.get_dummies(test_df[columns_to_encode], dtype=int)\n",
    "test_df_encode.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['org:resource', 'EventOrigin', 'EventID',\n",
    "       'lifecycle:transition', 'time:timestamp',\n",
    "       'case:ApplicationType', 'case:concept:name', 'case:RequestedAmount',\n",
    "       'FirstWithdrawalAmount', 'NumberOfTerms', 'MonthlyCost',\n",
    "       'Selected', 'CreditScore', 'OfferedAmount',\n",
    "       'event_index_in_trace', 'remaining_time', 'execution_time']\n",
    "\n",
    "# Concatenate the DataFrames based on the index\n",
    "full_train_df = pd.concat([train_df[columns_to_keep], train_df_encode], axis=1)\n",
    "full_test_df = pd.concat([test_df[columns_to_keep], test_df_encode], axis=1)\n",
    "\n",
    "full_train_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features X and targets y of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the one-hot encoded dataframes\n",
    "X_train = full_train_df.drop(columns=[\"remaining_time\"])\n",
    "X_train.to_csv(\"data/generated/onehot/X_train.csv\")\n",
    "\n",
    "X_test = full_test_df.drop(columns=[\"remaining_time\"])\n",
    "X_test.to_csv(\"data/generated/onehot/X_test.csv\")\n",
    "\n",
    "y_train = full_train_df[\"remaining_time\"]\n",
    "y_train.to_csv(\"data/generated/onehot/y_train.csv\")\n",
    "\n",
    "y_test = full_test_df[\"remaining_time\"]\n",
    "y_test.to_csv(\"data/generated/onehot/y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency encoding and save train test files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency encoding for test and train df\n",
    "# Select columns that start with \"concept:\" or \"Action_\"\n",
    "relevant_columns = [c for c in train_df_encode.columns if c.startswith(\"concept:\") or c.startswith(\"Action_\")]\n",
    "\n",
    "for trace_id, trace_df in full_train_df.groupby(\"case:concept:name\"):\n",
    "    trace_df_sorted = trace_df.sort_values(by='event_index_in_trace')\n",
    "    # Update only the selected columns with the cumulative sum\n",
    "    full_train_df.loc[trace_df_sorted.index, relevant_columns] = trace_df_sorted[relevant_columns].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns that start with \"concept:\" or \"Action_\"\n",
    "relevant_columns = [c for c in test_df_encode.columns if c.startswith(\"concept:\") or c.startswith(\"Action_\")]\n",
    "\n",
    "for trace_id, trace_df in full_test_df.groupby(\"case:concept:name\"):\n",
    "    trace_df_sorted = trace_df.sort_values(by='event_index_in_trace')\n",
    "    # Update only the selected columns with the cumulative sum\n",
    "    full_test_df.loc[trace_df_sorted.index, relevant_columns] = trace_df_sorted[relevant_columns].cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features X and targets y of train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the frequency encoded dataframes\n",
    "X_train = full_train_df.drop(columns=[\"remaining_time\"])\n",
    "X_train.to_csv(\"data/generated/frequency/X_train.csv\")\n",
    "\n",
    "X_test = full_test_df.drop(columns=[\"remaining_time\"])\n",
    "X_test.to_csv(\"data/generated/frequency/X_test.csv\")\n",
    "\n",
    "y_train = full_train_df[\"remaining_time\"]\n",
    "y_train.to_csv(\"data/generated/frequency/y_train.csv\")\n",
    "\n",
    "y_test = full_test_df[\"remaining_time\"]\n",
    "y_test.to_csv(\"data/generated/frequency/y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
